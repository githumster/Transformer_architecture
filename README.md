# PyTorch Transformer for Machine Translation

[![PyTorch](https://img.shields.io/badge/PyTorch-%23EE4C2C.svg?logo=PyTorch&logoColor=white)](https://pytorch.org/)

A PyTorch implementation of the Transformer architecture from "Attention Is All You You Need" (Vaswani et al., 2017) for machine translation tasks.

## Features

- Full Transformer architecture with encoder-decoder structure
- Multi-head attention with scaled dot-product
- Position-wise feed-forward networks
- Positional encoding for sequence order
- Residual connections and layer normalization
- Dropout for regularization
- Greedy decoding for inference
- Weight sharing between embeddings and final linear layer
- Xavier initialization for stable training
